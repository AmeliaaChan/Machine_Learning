## 生成式对抗网络GAN
### 一、基本概念介绍  
#### Generation
> 同时看x和z得到输出y  
> z从一个simple distribution中sample出来  
> 最后输出一个complex distribution  
+ **Why distribution?**  
"creativity"：同样的输入有多种可能的输出  
#### Generation Adversarial Network(GAN)  
+ **Uncinditional generation**  
去掉x，sample不同的z通过Generator得到不同的输出  
+ **Discriminator**  
输入图片，输出数值Scalar，Scalar越大越符合    
+ **Basic Idea of GAN**  
Adversarial：Generator和Discriminator之间存在对抗关系  
+ **Algorithm**  
假设G和D都有初始向量  
step1.定住G，用真正值和G产生的值训练D  
step2.定住D，升级G，让G试图骗过D，G调整参数让输出分数变大  
反复训练...  






### 二、理论介绍与WGAN  
#### 2.1 理论介绍
+ **Our Objective**  
输入一个Normal Distribution，生成$P_G$，真实的数据集对应$P_{data}$  
$$G^{*}=arg\ \underset {G}{min}Div(P_G,P_{data})$$  
Div：Divergence越小，两个Distribution越相似  
目标：找一组Generator里面的参数使得Divergence越小越好  
突破：只要能分别对PG和Pdata做sample，就可以估算出Div  
+ **Discriminator**  
Training：$D^{*}=arg\ \underset{D}{max}V(D,G)$  
Objective Function for D:$$V(G,D)=E_{y\sim P_{data}}[logD(y)]+E_{y\sim P_{G}}[log(1-D(y))]$$
希望V越大越好，即希望第一个D越大越好，第二个D越小越好  
发现：$\underset{D}{max}V(D,G)$与JS divergence有关  
  > 假设PG和Pdata的divergence很小，即二者很像
  对于Discriminator来讲，就很难将二者分开
  因此$\underset{D}{max}V(D,G)$就比较小
  小的divergence就对应的Discriminator的Objective Function的最大值也比较小  

  故，训练Discriminator，得到的Objective Function的最大值就和Divergence有关  
  $$G^{*}=arg\ \underset {G}{min}\ \underset{D}{max}V(D,G)$$  
  > 找D使得$V(D,G)$最大
  找G使得$\underset{D}{max}V(D,G)$最小  

#### 2.2 WGAN
+ **Tips for GAN**  
JS divergence：不合适。PG和Pdata的重叠范围其实很小，而对于两个不重合的distributions，JS divergence永远是log2  
Wasserstein distance：穷举Moving Plan，选择将P变成Q的最小平均推土距离  
+ **WGAN**  
$$\underset{D\in{1-Lipschitz}}{max}{\{E_{y\sim P_{data}}[D(y)]-E_{y\sim P_{G}}[D(y)]}\}$$  
D必须是足够平滑，E代表期望  






  


### 三、生成器效能评估与条件式生成
### 四、Cycle Gan